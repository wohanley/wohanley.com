<html>
  <head>
    <link rel="stylesheet" href="./essay.css" type="text/css"/>
  </head>
  <body>
    <article itemscope itemtype="http://schema.org/Article">
      <meta itemprop="identifier" content="tag:wohanley@wohanley.com,2024-06-14:bcli-ai-consultation">
      <meta itemprop="description" content="comments on the BC Law Institute's Consultation Paper on Artificial Intelligence and Civil Liability">
      <header>
        <h1>Re: Consultation Paper on Artificial Intelligence and Civil Liability</h1>
        <div class="subtitle">
          <time itemprop="issued" datetime="2024-06-14">14 June 2024</time><author>William O'Hanley</span>
        </div>
      </header>
      <section class="main-content">
        <div id="toc">
          <h3>Contents</h3>
        </div>
        <p>I was pleased to read the Consultation Paper published July 2023 by the Artificial Intelligence and Civil Liability Project Committee. I hope my comments contribute something helpful to the Committee's clearly thoughtful engagement with the topic.</p>
        <p>These comments can be summarized as follows:</p>
        <ul>
          <li>Rather than focusing on categorical definitions of particular systems as "AI", liability should arise from the denial of specific affordances, like transparency and oversight.</li>
          <li>It should not be possible to reduce one's exposure to liability by conducting similar activities but with less transparency or oversight.</li>
          <li>The situation with respect to algorithmic discrimination is already dire and threatens to become much worse. Discrimination may be inevitable when using certain technologies, and facilitating the use of such technologies should not be a priority.</li>
        </ul>
        <p>The early part of these comments focuses on the challenge of defining AI and reframes that (in my view, impossible) challenge in terms of identifying specific affordances of specific technologies, with particular regard to transparency, oversight, and contestability. Next, the comments move to analyzing why the Committee's recommendations pertaining to transparency and oversight, as they stand, will be difficult to apply in practice and may not address the real issues associated with AI, and why measures specifically targeting transparency and oversight as such could work better. The comments end with a discussion of algorithmic discrimination and an explanation of why, if we consider discriminatory systems without affordances for transparency and oversight rather than "AI" writ large, the de facto imposition of strict liability that arises from a straightforward application of the law of negligence is a proportionate response.</p>

        <h2 id="but-first-a-couple-of-quibbles">But First, a Couple of Quibbles</h2>
        <p>It's a mischaracterization to call GPT-3 "the prototype of ChatGPT" (page 39), GPT-3 and ChatGPT being categorically different products. Please excuse my pedantry; it's going to get much worse.</p>
        <p>"All forms of artificial intelligence employ algorithms" (page 10) is difficult to make sense of. All forms of software in general employ algorithms, and it's unclear what this means to say about AI in particular. This is especially confusing when it comes to statistical AI: the difficulty of conceptualizing statistical AIs as algorithmic, that is, of figuring out what procedural steps they're following, is precisely the source of their characteristic risks of unpredictability and inscrutability. Algorithms are buried in there somewhere, but that bare fact doesn't help us understand the system. I also observe a frequent confusion, when discussing "AI algorithms", between the algorithms used to conduct training and the inferred algorithms implicit in the resulting model. I suspect the discussion on page 10 will confuse more than it illuminates.</p>

        <h2 id="on-equivocation">On Equivocation</h2>
        <p>Much of the difficulty of finding definitions for AI and related concepts is due to widespread equivocation between the technical and vernacular senses of the words used. "Learning", for example, has its range of everyday meanings in cultural context, but as a term of art as in "machine learning", it refers to a very specific set of technical operations. It is common to mistakenly or deliberately discuss "learning" as if the tuning of weights in a neural network is actually substantially similar to human "learning", rather than very loosely analogical.
          <span class="footnote">Agre's discussion of “planning”, the term du jour in 1997, could be repeated almost verbatim about "learning" in 2023: ‘The strategic vagueness of AI vocabulary, and the use of technical schemata to narrate the operation of technical artifacts in intentional terms, is not a matter of conscious deception. It does permit AI's methods to seem broadly applicable, even when particular applications require a designer to make, often without knowing it, some wildly unreasonable assumptions. […] The term “planning”, in other words, exhibits an elastic quality: as a technical proposition it refers to a quite specific and circumscribed set of functionalities and algorithms, but as an empirical proposition it refers to anything at all that can plausibly be glossed with the term.’ <cite data-citekey="">Philip E. Agre, “Toward a Critical Technical Practice,” 1997, <a href="https://pages.gseis.ucla.edu/faculty/agre/critical.html">https://pages.gseis.ucla.edu/faculty/agre/critical.html</a>.</cite></span>
           We must be similarly wary of equivocation around the meanings of “training”, “experience”, “autonomy”, and so on. If I err toward excessive hand-wringing about language here, it is because this problem is so pervasive and so frequently encountered in the attempt to define AI, particularly here with regard to “decision-making”.</p>

        <h2 id="defining-ai">Defining AI</h2>
        <p>Because the Committee's recommendations apply to cases identified as involving AI, demonstrating that a particular system constitutes AI will be crucial to parties wishing to take advantage of the relevant protections, and this demonstration will not be realistically achievable in practice if the definition of AI is not given in very precise terms. I appreciate the Committee's efforts toward a functional definition less concerned with what AI <i>is</i> than what it <i>does</i>, and I am reaching for the same thing. The task is, I regret, not just difficult but impossible: AI cannot be defined in terms of function because "AI" is a marketing label.<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> Attempts to apply this same label to all of AlphaGo, COMPAS, Tay, self-driving cars, Deep Patient, ChatGPT, SyRI, and stock-trading algorithms cannot remain coherent, no matter how functional the definition; these technologies simply do not have enough in common.</p>
        <p>Further, while the "AI" label is stretched to breaking by the wide variety of technologies being included in it, it also doesn't capture everything we should think of as intelligence in the world of computation. Lewis et al. argue that Indigenous epistemologies are far better than Western for creating ethical relationships with non-human entities, regardless of their kind or degree of intelligence. In place of a firm definition of artificial intelligence, they propose 'an extended "circle of relationships" that includes the non-human kin&#x2014;from network daemons to robot dogs to artificial intelligences (AI) weak and, eventually, strong&#x2014;that increasingly populate our computational biosphere.' Their goal is that "we, as a species, figure out how to treat these new non-human kin respectfully and reciprocally&#x2014;and not as mere tools, or worse, slaves to their creators."<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup></p>
        <p>There are many intelligent systems in the computational world, and each demands (some quite urgently) a response tuned to the specific character of our relationships with it, and the label of "AI" does not help us to identify or respond to them. Instead of unworkable categorical assessments of whether a system is or is not AI, we can focus more productively on the specific affordances of specific technologies, that is, what actions the technologies leave open to you.<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup> We should be concerned with the particular affordances offered or withheld by systems regardless of whether they are marketed or generally understood as AI; the particular behaviours those affordances force, incentivize, or encourage on the part of developers, operators, and people subject to the operation; and the particular risks and harms associated with those behaviours.</p>
        <p>The Consultation Paper's working description of AI turns mainly on two features: autonomy and decision-making. It is difficult to reason about these features, and reframing the discussion in terms of affordances is more tractable.</p>
        <p>The autonomy of an AI system is not a quality of the system itself, but rather determined by the choices of its operator. Instead of examining the system to determine its inherent autonomy, we can ask what affordances of transparency and oversight the system makes available and what the operators do with them: How much effort does explaining the system's performance take, if it's possible at all? How much effort does the operator in fact put toward explaining the system's performance? How proactive is the operator in monitoring the system for harmful performance?</p>
        <p>Decision-making is also a matter of affordances, but it is difficult to see because the term is, as described above, the subject of frequent equivocation. The human activity we call "decision-making" is not the same thing as the automatic computational activity we call "decision-making": at the very least, human decision-making inevitably involves the exercise of normative judgment and computational "decision-making" does not (at least for the foreseeable future).<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup> Computational decision-making cannot "supplement" or "substitute for" human decision-making, it can only displace it.</p>
        <p>Rather than conducting an impossible search for decision-making capacity within a system to identify it as AI, it is more direct and more practical to examine the affordances the system offers in terms of contestability: Are the system's decisions immediately enacted, possibly by some physical process? Are they delayed? Are they relayed to a human? Are they reviewed? Can they be cancelled, overruled, or undone?</p>
        <p>The rise of AI does not introduce a new category of legal entity. Rather, it raises the profile of specific technological affordances or lack thereof, and these affordances are the proper subjects of a legal response. Deciding on what specific set of affordances is relevant would require some deliberation, and is beyond these comments: the main point is that truly functional definitions for AI-related concepts should be conceived in terms of affordances. Here I discuss AI mostly in terms of affordances of transparency and oversight, and their implications for contestability.</p>

        <h2 id="transparency">Transparency</h2>
        <p>A system that affords transparency is one that is well understood by the people who relate to it. This is, like autonomy, not a property that inheres in the system itself. Rather, it lies in the multiple understandings of various parties. The same system could be transparent to its creators and opaque to its end users, and many systems are. Transparency could be lacking because the system is impossible to explain, or it may simply be because no one has made the effort to explain. Many AI systems are opaque to everyone or almost everyone. The point here is that it is not possible to identify specific technical artifacts as transparent or opaque: it depends what people are doing with them.</p>
        <p>I read recommendations 5 and 6 as establishing a transparency mechanism:</p>
        <blockquote>
          <p>5. In an action for damages or other relief due to harm to persons or property arising from the operation of artificial intelligence, an obligation should rest on a defendant who developed the artificial intelligence system and made it available for use by that defendant or others to</p>
          <p>(a) provide an explanation for the performance of the system in the circumstances of the case that is consistent with the exercise of reasonable care in the design, development, training, and testing of the system; or</p>
          <p>(b) disclose sufficient information concerning the design and function of the system to all other parties before the trial of the action that will enable a party to determine the cause of the harmful performance of the system in the circumstances.</p>
          <p>6. If the cause of the harm is otherwise unexplained, and in the absence of an explanation or disclosure referred to in Recommendation 5, the court should be justified in drawing an inference of negligence against the defendants jointly and severally, except in regard to a defendant who rebuts the inference by evidence of the exercise of reasonable care by that defendant in the circumstances to avoid the harm suffered by the plaintiff.</p>
        </blockquote>
        <p>I do not expect that this obligation on developers would be practicable, nor that it would provide adequate transparency if it were.</p>
        <p>Firstly, producing or understanding explanations of the performance of certain kinds of AI systems, where it is possible at all, requires rare and considerable technical skill. A company like, for example, OpenAI, with massively complex models and a small number of developers relative to operators, could be obliged to provide many more explanations than its staff can support. Such companies may have no realistic option other than to publicly disclose enough information to comply with (b) in all conceivable circumstances, which raises its own problems. The company may not have enough information to enable anyone, much less a non-technical party, to determine the cause of a harmful behaviour - the technology to do so simply may not exist.</p>
        <p>Regardless of our regulatory approach, we will eventually wind up in a similar predicament. For many AI systems, the brute fact of the matter is that vastly more capacity exists to produce output than to explain it. There is today no realistic prospect of obtaining specific explanations understandable to non-experts for every legally dubious output of, for example, GPT-4.</p>
        <p>Secondly, even if it were possible to obtain specific explanations on demand, many AI systems would still be significantly opaque. There is a separate and potentially harder problem of knowing when AI is involved in your life at all, or of what its effects are on you. For example, before the investigations that revealed the discriminatory effects of COMPAS, a Black defendant simply could not have known that they were a victim of AI-caused harm, or indeed any harm at all. van den Hoven describes "hermeneutical injustice", this deprivation of disadvantaged parties of "the conceptual and interpretive tools needed to make sense of their experiences", as an important kind of injustice in and of itself, even before considering its effects on one's ability to assert their legal rights.<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>^{,}<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup> Further, many AI systems force their users into restrictive end-user licence agreements that prohibit scraping or otherwise probing the behaviour of the AI, such that developing an understanding of the AI is not just inherently difficult but also, practically speaking, contractually forbidden.</p>
        <p>The vast majority of people, or in some cases all people, cannot expect to comprehend the workings of certain AI systems. Our legal approach, therefore, cannot depend on our comprehending them. Systems lacking affordances of transparency, whether intentionally or because of technical impossibility, are a more precise target than AI. For example, in line with the Committee's consideration of AI in terms of product liability, Bednar suggests that products incorporating AI should be labelled as such in compliance with the <i>Consumer Packaging and Labelling Act</i>.<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup> What might the law consider to be adequate transparency for automated systems, sufficient to meet the standard of care? Is providing adequate transparency a component of AI operators' duty of care? What legal protections are necessary for parties who are denied transparency into the systems that affect them? Responses along these lines are more likely to address the actual problems with AI and to retain their usefulness as new technologies emerge.</p>

        <h2 id="oversight">Oversight</h2>
        <p>Closely related to transparency are affordances of oversight. A system's performance may be theoretically understandable, but to what extent is it actually understood and explained in practice? To what extent does the operator actively observe the system in use? To what extent can the operator, or another affected party, intervene? In terms of affordances, concerns about autonomy are shifted toward concerns about oversight. Autonomy is thus not a property of the system itself, but a matter of how the operator chooses to use it. With respect to negligence, at what point does a failure to provide oversight constitute a breach of the standard of care? What legal protections are appropriate for people affected by a system operated without adequate oversight?</p>

        <p>Diver points out the necessity of oversight to regulatory compliance: if a court orders performance on the part of a system's developers or operators in order to remedy some illegality, and it is technically impossible for them to comply, what does that say about the ex ante legitimacy of the system?<sup><a id="fnr.9" class="footref" href="#fn.9" role="doc-backlink">9</a></sup> To what extent should parties be liable for deploying such systems in the first place? The question here is, again, not what rules should exist for AI specifically, but the risks arising from systems that lack affordances for meaningful oversight, and what constraints the law should place on the operation of such systems. In particular, statistical AI systems lacking transparency and oversight present enormous risks of discrimination.</p>

        <h2 id="discrimination">Discrimination</h2>
        <p>Discrimination is rampant in predictive analytics, including statistical AI. Statistical AI does not merely carry a risk of discrimination, it discriminates as a matter of course.<sup><a id="fnr.10" class="footref" href="#fn.10" role="doc-backlink">10</a></sup>^{,}<sup><a id="fnr.11" class="footref" href="#fn.11" role="doc-backlink">11</a></sup> For the large majority of practical use cases, there are no unbiased datasets anywhere in the world, and therefore <i>it is impossible to produce an unbiased statistical AI</i>.<sup><a id="fnr.12" class="footref" href="#fn.12" role="doc-backlink">12</a></sup> Given what is presently known about the nature of statistical AI and the lack of effective strategies for mitigating their biases, the assumption should be that statistical AIs are discriminatory and the burden should be on their developers and operators to prove otherwise. If such proof is impractical, that is all the more reason the product should not be developed or released.</p>
        <p>Recommendation 9 reads:</p>
        <blockquote>
          <p>9. Failure to take reasonable steps to detect and correct biased output of an artificial intelligence system, resulting in discrimination against a person or class that is not warranted by reasonable business or industry practice, or is contrary to law, should amount to actionable negligence if the differential treatment of the plaintiff resulting from the output is accompanied by actual damage.</p>
        </blockquote>
        <p>The first problem is that to "detect and correct biased output" is not in fact possible. While numerous techniques have been developed under the banner of "algorithmic fairness", some of which the Committee points to, there is limited evidence that they have been effective in practice.<sup><a id="fnr.13" class="footref" href="#fn.13" role="doc-backlink">13</a></sup>^{,}<sup><a id="fnr.14" class="footref" href="#fn.14" role="doc-backlink">14</a></sup> It would be extremely useful if there were a set of anti-discrimination best practices for AI that were effective enough that following them would constitute reasonable behaviour. Unfortunately, there are no such practices today and it is not clear that any will ever emerge.</p>
        <p>The second problem is that actual damage caused by algorithmic discrimination will often be virtually impossible not only to prove, but to attribute to the relevant AI system in the first place, for reasons related to hermeneutical injustice as discussed above. For a wronged party to even suspect algorithmic discrimination as the cause of their harm, they would need a broad awareness of the AI systems they interact with in the course of their life, as well as relatively advanced knowledge of the workings of those systems. In most cases, parties will have no idea of what AI is being used for and how it is affecting them.<sup><a id="fnr.15" class="footref" href="#fn.15" role="doc-backlink">15</a></sup> Were you harmed by AI 1 that didn't show you advertisements for well-paying job opportunities, or by AI 2 that calculated it would be more profitable to show you ads for airplane tickets, or AI 3 that classified you as interested in travel and shared that with AI 2? Or is the harm more related to the fact that the apparently competing social networks you use are, behind the scenes, all relying on AI 1? How do you even know any of these AIs or arrangements exist, if all that happened was that there were some advertisements you never saw? As hard as it is, demonstrating a link between some particular AI and actual damage is the easy part.</p>
        <p>AI does not just complicate the law of negligence, it invalidates its premises. As previously stressed, AI "decision-making" is a categorically different activity to human "decision-making", and cannot be evaluated on the same terms. Algorithmic discrimination is not checked by the normative judgment of human beings, is based on unknown or unknowable grounds, cannot be evaluated by the standards of reasonableness, and operates at a scale so much larger than human discrimination as to be of a qualitatively different kind. The law of negligence buckles under the weight of algorithmic discrimination.</p>

        <h2 id="liability">Liability</h2>
        <p>I hope that I have laid enough of a foundation to now explain why I believe the law of negligence, even with the modifications proposed by the Committee, is insufficient to protect against the risks created by certain AI systems.</p>
        <p>First, as the Committee notes, the standard of human reasonableness cannot be applied coherently to the behaviour of non-human actors, and the result if this standard were applied would be the de facto imposition of strict liability on AI operators. I agree with this reasoning, but I have no qualms about accepting the implication that strict liability is the appropriate response. Point by point (pages 36-37):</p>
        <ul>
          <li>I agree that <i>"Unpredictable results based on machine learning are not equivalent to the escape of a wild animal or a noxious substance,"</i> but in the opposite sense to the Committee: I believe they are far worse. Algorithmic discrimination is an extremely serious risk that operates at a scale vastly beyond a single animal.<br /></li>
          <li><i>"Many artificial intelligence systems serve to reduce or eliminate risk, which implies that their autonomy should be seen as a valuable quality rather than a source of danger in most cases."</i> I confess to being a bit confused by this argument. If such systems do genuinely reduce risk, they will correspondingly reduce the likelihood of liability. If they do not, then there is no reason to facilitate their operation.<br /></li>
          <li><i>"Treating artificial intelligence as an intrinsically dangerous thing, which is the effect of blanket adoption of strict liability, would foster a liability and financial risk climate that would discourage development and be counter-productive. It is misguided to treat the deployment of artificial intelligence as tortious in itself."</i> This is one of the reasons I would not recommend trying to define AI at all, much less as intrinsically dangerous. There are certain systems in operation which are based on technologies known to be virtually always discriminatory and which offer almost no affordances of transparency or oversight. Such systems are intrinsically dangerous and there is no reason to encourage their development. This argument has no bearing on other systems that may also be marketed as AI but which do not have the same characteristics.<br /></li>
          <li>On the question of insurance, I would suggest that if an insurer cannot be convinced to assume the risk of harm in exchange for a premium, the public should not be made to assume that risk for free. This is especially so when the public benefits of the kinds of AI systems that would be difficult to insure (opaque, unsupervised, discriminatory) remain dubious.<br /></li>
          <li><i>"If firms know they will be held liable regardless of the degree of care they exercise, they will not have an incentive to raise and continually improve standards and practices that will increase safety."</i> If firms know they will be held liable for harm regardless of their degree of care, they have an incentive to avoid harming people. They may do this by any means they like, including by the development of standards and practices, if those do in fact work to reduce the rate of failure. Given the current state of industry standards, and the lack of evidence that they actually prevent harm, there is little reason to allow firms to avoid liability by following them.<br /></li>
        </ul>
        <p>Imposing strict liability is the natural conclusion of applying the existing law of negligence to inherently discriminatory systems that lack transparency and oversight. If decisions are made by some process which cannot be evaluated in terms of reasonableness, but which is known to often produce results that do not accord with what reasonable humans would produce, the law ought to assume the decisions are unreasonable rather than exempt them from the requirement of reasonableness altogether. I hope that the conclusion in these terms is less distasteful to the Committee than the idea of "deployment of artificial intelligence as tortious in itself."</p>
        <p>In particular, this liability is avoidable: no one is forced to use AI in a way that involves relying on it to make reasonable decisions. Operators can avoid technologies that lack affordances of transparency. If they must use such technologies, they can provide the forms of oversight necessary to prevent unreasonable decisions from taking effect. If oversight is also impossible, then the operator will have to decide whether the benefits of the system outweigh its risks of harm, and bear those risks if so. Concluding otherwise would allow firms to avoid liability for unreasonable decisions by simply having a machine make those decisions for them.</p>
        <p>It is difficult to overstate my concern about algorithmic discrimination, and I do not think that the de facto imposition of strict liability is too severe a measure to manage it. I appreciate that for some firms developing and deploying AI products, there may be no reasonable steps they can take to provide transparency and oversight or to avoid discrimination. I am troubled less by the thought of what will happen if these products do not come to market than by the thought of what will happen if they do.</p>

        <div id="footnotes">
          <h2 class="footnotes">Footnotes: </h2>

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The Center on Privacy &amp; Technology at Georgetown Law refuses to use the terms "AI" or "machine learning" in its work for this reason. Emily Tucker, “Artifice and Intelligence” (Tech Policy Press, March 17, 2022), <a href="https://techpolicy.press/artifice-and-intelligence/">https://techpolicy.press/artifice-and-intelligence/</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Jason Edward Lewis et al., “Making Kin with the Machines,” <i>Journal of Design and Science</i>, July 16, 2018, <a href="https://doi.org/10.21428/bfafd97b">https://doi.org/10.21428/bfafd97b</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Diver defines the idea of "digisprudence" in such terms, considering the legitimacy of digital systems by mapping their affordances (or lack thereof) to legisprudential principles like alternativity, temporality, and coherence. The overarching digisprudential concern is contestability: "it must in the end always be possible for the user to resort to a court action to determine illegality of whatever substantive form." Laurence Diver, “Digisprudence: The Design of Legitimate Code,” <i>Law, Innovation and Technology</i> 13, no. 2 (July 3, 2021): 33, <a href="https://doi.org/10.1080/17579961.2021.1977217">https://doi.org/10.1080/17579961.2021.1977217</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Differentiating the human decision-making process from the computational is the subject of a great deal of research. In the specific context of AI, Weizenbaum was one of the early explorers of the topic Joseph Weizenbaum, <i>Computer Power and Human Reason: From Judgment to Calculation</i> (San Francisco: Freeman, 1976).. Hildebrandt discusses these issues in a more modern and specifically legal context Mireille Hildebrandt, “Law As Computation in the Era of Artificial Legal Intelligence. Speaking Law to the Power of Statistics,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, June 7, 2017), <a href="https://papers.ssrn.com/abstract=2983045">https://papers.ssrn.com/abstract=2983045</a>..</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Emilie van den Hoven, “Hermeneutical Injustice and the Computational Turn in Law,” <i>Journal of Cross-Disciplinary Research in Computational Law</i> 1, no. 1 (March 23, 2021), <a href="https://journalcrcl.org/crcl/article/view/6">https://journalcrcl.org/crcl/article/view/6</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Whaanga writes of current AI deployments as deeply implicated in ongoing colonial efforts to displace Indigenous peoples' worldviews and tools of self-definition in favour of colonizers'. Hēmi Whaanga, “AI: A New (R)Evolution or the New Colonizer for Indigenous Peoples?,” in <i>Indigenous Protocol and Artificial Intelligence Position Paper</i>, by Angie Abdilla et al. (Aboriginal Territories in Cyberspace, 2020), 34–38, <a href="https://spectrum.library.concordia.ca/id/eprint/986506">https://spectrum.library.concordia.ca/id/eprint/986506</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Vass Bednar, “What Tech Policy Can Learn from Textiles” (Regs to Riches, February 15, 2023), <a href="https://www.regs2riches.com/p/what-tech-policy-can-learn-from-textiles">https://www.regs2riches.com/p/what-tech-policy-can-learn-from-textiles</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Diver, “Digisprudence.”</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10" role="doc-backlink">10</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Angelina Wang et al., “Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms That Optimize Predictive Accuracy,” SSRN Scholarly Paper, October 2022, <a href="https://papers.ssrn.com/abstract=4238015">https://papers.ssrn.com/abstract=4238015</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.11" class="footnum" href="#fnr.11" role="doc-backlink">11</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Former data scientist Os Keyes calls data science "a path to cultural genocide" for queer people: "gender-recognition systems are fundamentally controlling and dangerous to trans people and simply cannot be reformed to not be violent against us. Normalizing, reductive views of gender are <i>what these systems are for</i>." Os Keyes, “Counting the Countless,” <i>Real Life</i>, April 8, 2019, <a href="https://reallifemag.com/counting-the-countless/">https://reallifemag.com/counting-the-countless/</a>..</p></div></div>

<div class="footdef"><sup><a id="fn.12" class="footnum" href="#fnr.12" role="doc-backlink">12</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
An AI can be no less biased than its constitutive data. As Hong writes, "To suggest that the neutrality of data or algorithm cleanses predictions of their own historical provenance effectively describes a variation of money laundering, in which decisions like what kind of data is and is not gathered, or where the resulting 'insights' are deployed, are written off as someone else's problem." Sun-ha Hong, “Prediction as Extraction of Discretion,” <i>Big Data &#38; Society</i> 10, no. 1 (January 1, 2023): 4, <a href="https://doi.org/10.1177/20539517231171053">https://doi.org/10.1177/20539517231171053</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.13" class="footnum" href="#fnr.13" role="doc-backlink">13</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Ellen P. Goodman and Julia Trehu, “AI Audit Washing and Accountability,” SSRN Scholarly Paper (Rochester, NY, September 22, 2022), <a href="https://doi.org/10.2139/ssrn.4227350">https://doi.org/10.2139/ssrn.4227350</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.14" class="footnum" href="#fnr.14" role="doc-backlink">14</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
  Even in a theoretical sense, there is a strong case to be made that algorithmic fairness is impossible by definition. As Hu writes, work on algorithmic fairness "casts a normative dispute about what kinds of actions should be deemed discriminatory as an empirical dispute about causes." Lily Hu, “Direct Effects” (Phenomenal World, September 25, 2020), <a href="https://www.phenomenalworld.org/analysis/direct-effects/">https://www.phenomenalworld.org/analysis/direct-effects/</a>. Computers cannot "solve" discrimination, because they cannot understand the underlying social realities that make it a problem. Grant and Chu cogently describe the problem of casting fairness in statistical terms and suggest that "perhaps formal approaches to fairness are misguided. Instead of focusing on the statistics, perhaps we should focus on the broader social and political implications of using automated prediction tools." Cosmo Grant and Emily Chu, “Is It Impossible to Be Fair?” (Jain Family Institute, August 23, 2019), <a href="https://jainfamilyinstitute.github.io/algorithmic-fairness/">https://jainfamilyinstitute.github.io/algorithmic-fairness/</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.15" class="footnum" href="#fnr.15" role="doc-backlink">15</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
  Zeide describes how the harms of algorithmic discrimination are invisibly distributed throughout the platform economy, and how these patterns do not fit regulatory regimes focused on individual decisions or decision-makers Elana Zeide, “The Silicon Ceiling: How Artificial Intelligence Constructs an Invisible Barrier to Opportunity,” SSRN Scholarly Paper (Rochester, NY, January 5, 2023), <a href="https://papers.ssrn.com/abstract=4318860">https://papers.ssrn.com/abstract=4318860</a>..</p></div></div>


        </div>
    </article>
  </body>
</html>
